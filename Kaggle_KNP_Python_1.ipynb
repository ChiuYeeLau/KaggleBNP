{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Feb  4 06:20:12 2016\n",
    "\n",
    "@author: monkey\n",
    "\"\"\"\n",
    "# % pylab inline \n",
    "# % matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import csv\n",
    "import numpy as np\n",
    "import pdb\n",
    "from copy import deepcopy\n",
    "import random\n",
    "# import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "# from bayes_opt import BayesianOptimization\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn import ensemble\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data...\n"
     ]
    }
   ],
   "source": [
    "print('Load data...')\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "target = train['target']\n",
    "\n",
    "train_with_target = train.drop(['ID'], axis = 1)\n",
    "train = train.drop(['ID', 'target'], axis = 1)\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "ids = test['ID'].values\n",
    "test = test.drop(['ID'], axis = 1)\n",
    "\n",
    "# add number of nans\n",
    "train['number_NAN'] = train.isnull().sum(axis = 1)\n",
    "test['number_NAN'] = test.isnull().sum(axis = 1)\n",
    "train_with_target['number_NAN'] = train.isnull().sum(axis = 1)\n",
    "\n",
    "train_len = train.shape[0]\n",
    "test_len = test.shape[0]\n",
    "\n",
    "for elt in train.columns:\n",
    "    train[elt + '_na'] = pd.isnull(train[elt]).astype(int)\n",
    "    test[elt + '_na'] = pd.isnull(test[elt]).astype(int)\n",
    "\n",
    "train = train.fillna(-1)\n",
    "test = test.fillna(-1)\n",
    "train_with_target = train_with_target.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "264"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.columns == test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v123_na</th>\n",
       "      <th>v124_na</th>\n",
       "      <th>v125_na</th>\n",
       "      <th>v126_na</th>\n",
       "      <th>v127_na</th>\n",
       "      <th>v128_na</th>\n",
       "      <th>v129_na</th>\n",
       "      <th>v130_na</th>\n",
       "      <th>v131_na</th>\n",
       "      <th>number_NAN_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.503281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.312910</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>8.965516</td>\n",
       "      <td>6.542669</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2 v3        v4         v5        v6        v7        v8  \\\n",
       "0  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895  0.012941   \n",
       "1 -1.000000 -1.000000  C -1.000000   9.191265 -1.000000 -1.000000  2.301630   \n",
       "2  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571  0.019645   \n",
       "3  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549  0.171947   \n",
       "4 -1.000000 -1.000000  C -1.000000  -1.000000 -1.000000 -1.000000 -1.000000   \n",
       "\n",
       "          v9       v10      ...        v123_na  v124_na  v125_na  v126_na  \\\n",
       "0   9.999999  0.503281      ...              0        0        0        0   \n",
       "1  -1.000000  1.312910      ...              1        0        0        1   \n",
       "2  12.666667  0.765864      ...              0        0        0        0   \n",
       "3   8.965516  6.542669      ...              0        0        0        0   \n",
       "4  -1.000000  1.050328      ...              1        1        0        1   \n",
       "\n",
       "   v127_na  v128_na  v129_na  v130_na  v131_na  number_NAN_na  \n",
       "0        0        0        0        0        0              0  \n",
       "1        1        0        0        1        1              0  \n",
       "2        0        0        0        0        0              0  \n",
       "3        0        0        0        0        0              0  \n",
       "4        1        1        0        1        1              0  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# filter all object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "object_features = []\n",
    "all_features = []\n",
    "for feat in train.columns:\n",
    "    if feat != 'number_NAN':\n",
    "        all_features.append(feat)\n",
    "    if train[feat].dtype == 'object':\n",
    "        object_features.append(feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['v3', 'v22', 'v24', 'v30', 'v31', 'v47', 'v52', 'v56', 'v66', 'v71', 'v74', 'v75', 'v79', 'v91', 'v107', 'v110', 'v112', 'v113', 'v125', 'v38', 'v62', 'v72', 'v129']\n"
     ]
    }
   ],
   "source": [
    "object_features.extend(['v38', 'v62', 'v72', 'v129'])\n",
    "print object_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = all_features[:131]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add frequencies for object features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'v1', u'v2', u'v3', u'v4', u'v5', u'v6', u'v7', u'v8', u'v9', u'v10',\n",
       "       ...\n",
       "       u'v123_na', u'v124_na', u'v125_na', u'v126_na', u'v127_na', u'v128_na',\n",
       "       u'v129_na', u'v130_na', u'v131_na', u'number_NAN_na'],\n",
       "      dtype='object', length=264)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_categories = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_dummies = pd.DataFrame()\n",
    "test_dummies = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prob = train_with_target.groupby(['target']).size()[1]/float(sum(train_with_target.groupby(['target']).size()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.76119872989214576"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# try TSNE!!!!!!!!!!!!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v123_na</th>\n",
       "      <th>v124_na</th>\n",
       "      <th>v125_na</th>\n",
       "      <th>v126_na</th>\n",
       "      <th>v127_na</th>\n",
       "      <th>v128_na</th>\n",
       "      <th>v129_na</th>\n",
       "      <th>v130_na</th>\n",
       "      <th>v131_na</th>\n",
       "      <th>number_NAN_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.335739</td>\n",
       "      <td>8.727474</td>\n",
       "      <td>C</td>\n",
       "      <td>3.921026</td>\n",
       "      <td>7.915266</td>\n",
       "      <td>2.599278</td>\n",
       "      <td>3.176895</td>\n",
       "      <td>0.012941</td>\n",
       "      <td>9.999999</td>\n",
       "      <td>0.503281</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>9.191265</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2.301630</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.312910</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.943877</td>\n",
       "      <td>5.310079</td>\n",
       "      <td>C</td>\n",
       "      <td>4.410969</td>\n",
       "      <td>5.326159</td>\n",
       "      <td>3.979592</td>\n",
       "      <td>3.928571</td>\n",
       "      <td>0.019645</td>\n",
       "      <td>12.666667</td>\n",
       "      <td>0.765864</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.797415</td>\n",
       "      <td>8.304757</td>\n",
       "      <td>C</td>\n",
       "      <td>4.225930</td>\n",
       "      <td>11.627438</td>\n",
       "      <td>2.097700</td>\n",
       "      <td>1.987549</td>\n",
       "      <td>0.171947</td>\n",
       "      <td>8.965516</td>\n",
       "      <td>6.542669</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2 v3        v4         v5        v6        v7        v8  \\\n",
       "0  1.335739  8.727474  C  3.921026   7.915266  2.599278  3.176895  0.012941   \n",
       "1 -1.000000 -1.000000  C -1.000000   9.191265 -1.000000 -1.000000  2.301630   \n",
       "2  0.943877  5.310079  C  4.410969   5.326159  3.979592  3.928571  0.019645   \n",
       "3  0.797415  8.304757  C  4.225930  11.627438  2.097700  1.987549  0.171947   \n",
       "4 -1.000000 -1.000000  C -1.000000  -1.000000 -1.000000 -1.000000 -1.000000   \n",
       "\n",
       "          v9       v10      ...        v123_na  v124_na  v125_na  v126_na  \\\n",
       "0   9.999999  0.503281      ...              0        0        0        0   \n",
       "1  -1.000000  1.312910      ...              1        0        0        1   \n",
       "2  12.666667  0.765864      ...              0        0        0        0   \n",
       "3   8.965516  6.542669      ...              0        0        0        0   \n",
       "4  -1.000000  1.050328      ...              1        1        0        1   \n",
       "\n",
       "   v127_na  v128_na  v129_na  v130_na  v131_na  number_NAN_na  \n",
       "0        0        0        0        0        0              0  \n",
       "1        1        0        0        1        1              0  \n",
       "2        0        0        0        0        0              0  \n",
       "3        0        0        0        0        0              0  \n",
       "4        1        1        0        1        1              0  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v3</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>...</th>\n",
       "      <th>v123_na</th>\n",
       "      <th>v124_na</th>\n",
       "      <th>v125_na</th>\n",
       "      <th>v126_na</th>\n",
       "      <th>v127_na</th>\n",
       "      <th>v128_na</th>\n",
       "      <th>v129_na</th>\n",
       "      <th>v130_na</th>\n",
       "      <th>v131_na</th>\n",
       "      <th>number_NAN_na</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.375465e+00</td>\n",
       "      <td>11.361141</td>\n",
       "      <td>C</td>\n",
       "      <td>4.200778</td>\n",
       "      <td>6.57700</td>\n",
       "      <td>2.081784</td>\n",
       "      <td>1.784386</td>\n",
       "      <td>0.011094</td>\n",
       "      <td>9.523810</td>\n",
       "      <td>1.312911</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>C</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.00000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.291029</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4.903407e-07</td>\n",
       "      <td>8.201529</td>\n",
       "      <td>C</td>\n",
       "      <td>4.544371</td>\n",
       "      <td>6.55010</td>\n",
       "      <td>1.558442</td>\n",
       "      <td>2.467532</td>\n",
       "      <td>0.007164</td>\n",
       "      <td>7.142858</td>\n",
       "      <td>1.575492</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.661870e+00</td>\n",
       "      <td>3.041241</td>\n",
       "      <td>C</td>\n",
       "      <td>1.657216</td>\n",
       "      <td>9.77308</td>\n",
       "      <td>2.078337</td>\n",
       "      <td>1.430855</td>\n",
       "      <td>1.252157</td>\n",
       "      <td>7.959596</td>\n",
       "      <td>1.575493</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.252822e+00</td>\n",
       "      <td>11.283352</td>\n",
       "      <td>C</td>\n",
       "      <td>4.638388</td>\n",
       "      <td>8.52051</td>\n",
       "      <td>2.302484</td>\n",
       "      <td>3.510159</td>\n",
       "      <td>0.074263</td>\n",
       "      <td>7.612904</td>\n",
       "      <td>1.050328</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 264 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             v1         v2 v3        v4       v5        v6        v7  \\\n",
       "0  1.375465e+00  11.361141  C  4.200778  6.57700  2.081784  1.784386   \n",
       "1 -1.000000e+00  -1.000000  C -1.000000 -1.00000 -1.000000 -1.000000   \n",
       "2 -4.903407e-07   8.201529  C  4.544371  6.55010  1.558442  2.467532   \n",
       "3  2.661870e+00   3.041241  C  1.657216  9.77308  2.078337  1.430855   \n",
       "4  1.252822e+00  11.283352  C  4.638388  8.52051  2.302484  3.510159   \n",
       "\n",
       "         v8        v9       v10      ...        v123_na  v124_na  v125_na  \\\n",
       "0  0.011094  9.523810  1.312911      ...              0        0        0   \n",
       "1 -1.000000 -1.000000  1.291029      ...              1        1        0   \n",
       "2  0.007164  7.142858  1.575492      ...              0        0        0   \n",
       "3  1.252157  7.959596  1.575493      ...              0        0        0   \n",
       "4  0.074263  7.612904  1.050328      ...              0        0        0   \n",
       "\n",
       "   v126_na  v127_na  v128_na  v129_na  v130_na  v131_na  number_NAN_na  \n",
       "0        0        0        0        0        0        0              0  \n",
       "1        1        1        1        0        1        1              0  \n",
       "2        0        0        0        0        0        0              0  \n",
       "3        0        0        0        0        0        0              0  \n",
       "4        0        0        0        0        0        0              0  \n",
       "\n",
       "[5 rows x 264 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v1\n",
      "v2\n",
      "v3\n",
      "v4\n",
      "v5\n",
      "v6\n",
      "v7\n",
      "v8\n",
      "v9\n",
      "v10\n",
      "v11\n",
      "v12\n",
      "v13\n",
      "v14\n",
      "v15\n",
      "v16\n",
      "v17\n",
      "v18\n",
      "v19\n",
      "v20\n",
      "v21\n",
      "v22\n",
      "v23\n",
      "v24\n",
      "v25\n",
      "v26\n",
      "v27\n",
      "v28\n",
      "v29\n",
      "v30\n",
      "v31\n",
      "v32\n",
      "v33\n",
      "v34\n",
      "v35\n",
      "v36\n",
      "v37\n",
      "v38\n",
      "v39\n",
      "v40\n",
      "v41\n",
      "v42\n",
      "v43\n",
      "v44\n",
      "v45\n",
      "v46\n",
      "v47\n",
      "v48\n",
      "v49\n",
      "v50\n",
      "v51\n",
      "v52\n",
      "v53\n",
      "v54\n",
      "v55\n",
      "v56\n",
      "v57\n",
      "v58\n",
      "v59\n",
      "v60\n",
      "v61\n",
      "v62\n",
      "v63\n",
      "v64\n",
      "v65\n",
      "v66\n",
      "v67\n",
      "v68\n",
      "v69\n",
      "v70\n",
      "v71\n",
      "v72\n",
      "v73\n",
      "v74\n",
      "v75\n",
      "v76\n",
      "v77\n",
      "v78\n",
      "v79\n",
      "v80\n",
      "v81\n",
      "v82\n",
      "v83\n",
      "v84\n",
      "v85\n",
      "v86\n",
      "v87\n",
      "v88\n",
      "v89\n",
      "v90\n",
      "v91\n",
      "v92\n",
      "v93\n",
      "v94\n",
      "v95\n",
      "v96\n",
      "v97\n",
      "v98\n",
      "v99\n",
      "v100\n",
      "v101\n",
      "v102\n",
      "v103\n",
      "v104\n",
      "v105\n",
      "v106\n",
      "v107\n",
      "v108\n",
      "v109\n",
      "v110\n",
      "v111\n",
      "v112\n",
      "v113\n",
      "v114\n",
      "v115\n",
      "v116\n",
      "v117\n",
      "v118\n",
      "v119\n",
      "v120\n",
      "v121\n",
      "v122\n",
      "v123\n",
      "v124\n",
      "v125\n",
      "v126\n",
      "v127\n",
      "v128\n",
      "v129\n",
      "v130\n",
      "v131\n"
     ]
    }
   ],
   "source": [
    "for elt in all_features:\n",
    "    vector = pd.concat([train[elt], test[elt]], axis = 0)\n",
    "    print str(elt)\n",
    "\n",
    "    if len(vector.unique()) <= max_categories:\n",
    "        train_dummies = pd.concat([train_dummies, pd.get_dummies(train[elt], prefix = elt, \\\n",
    "                                                                 dummy_na = True)], axis = 1).astype('int8')\n",
    "        test_dummies = pd.concat([test_dummies, pd.get_dummies(test[elt], prefix = elt, \\\n",
    "                                                               dummy_na = True)], axis = 1).astype('int8')\n",
    "        \n",
    "        agrg = pd.concat([train[elt], target], axis = 1)\n",
    "        agrg = agrg.groupby(by = elt, axis = 0).agg(['sum','count']).target\n",
    "        agrg['weight'] = agrg.apply(lambda x: .5 + .5 * x['sum']/x['count'] if\\\n",
    "                                    (x['sum'] > x['count'] - x['sum']) else .5 + .5 * \\\n",
    "                                    (x['sum'] - x['count'])/x['count'], axis = 1)\n",
    "        agrg.reset_index(inplace = True)\n",
    "        train[elt + '_weight'] = pd.merge(train, agrg, how = 'left', on = elt)['weight']\n",
    "        test[elt + '_weight'] = pd.merge(test, agrg, how = 'left', on = elt)['weight']\n",
    "        train[elt + '_weight'] = train[elt + '_weight'].fillna(prob)\n",
    "        test[elt + '_weight'] = test[elt + '_weight'].fillna(prob)\n",
    "        \n",
    "        del train[elt], test[elt]\n",
    "        # train[elt], tmp_indexer = pd.factorize(train[elt])\n",
    "        # test[elt] = tmp_indexer.get_indexer(test[elt])\n",
    "    else:\n",
    "        typ = str(train[elt].dtype)[:3]\n",
    "        if (typ == 'flo') or (typ == 'int'):\n",
    "            \n",
    "            train[elt] = np.log(2.0 + train[elt])\n",
    "            test[elt] = np.log(2.0 + test[elt])\n",
    "        else:\n",
    "            if (typ == 'obj'):\n",
    "                \n",
    "                # del train[elt], test[elt]\n",
    "                train[elt + '_binary'], tmp_indexer = pd.factorize(train[elt])\n",
    "                test[elt + '_binary'] = tmp_indexer.get_indexer(test[elt])\n",
    "                \n",
    "                list2keep = vector.value_counts()[ : max_categories].index\n",
    "                train[elt] = train[elt].apply(lambda x: x if x in list2keep else np.nan)\n",
    "                test[elt] = test[elt].apply(lambda x: x if x in list2keep else np.nan)                \n",
    "                train_dummies = pd.concat([train_dummies, pd.get_dummies(train[elt], prefix = elt, \\\n",
    "                                                                         dummy_na = True)], axis = 1).astype('int8')\n",
    "                test_dummies = pd.concat([test_dummies, pd.get_dummies(test[elt], prefix = elt, \\\n",
    "                                                                       dummy_na = True)], axis = 1).astype('int8')\n",
    "                \n",
    "                agrg = pd.concat([train[elt], target], axis = 1)\n",
    "                agrg = agrg.groupby(by = elt, axis = 0).agg(['sum','count']).target\n",
    "                agrg['weight'] = agrg.apply(lambda x: .5 + .5 * x['sum']/x['count'] if\\\n",
    "                                            (x['sum'] > x['count'] - x['sum']) else .5 + .5 * \\\n",
    "                                            (x['sum'] - x['count'])/x['count'], axis = 1)\n",
    "                agrg.reset_index(inplace = True)\n",
    "                train[elt + '_weight'] = pd.merge(train, agrg, how = 'left', on = elt)['weight']\n",
    "                test[elt + '_weight'] = pd.merge(test, agrg, how = 'left', on = elt)['weight']\n",
    "                train[elt + '_weight'] = train[elt + '_weight'].fillna(prob)\n",
    "                test[elt + '_weight'] = test[elt + '_weight'].fillna(prob)\n",
    "                \n",
    "                del train[elt], test[elt]\n",
    "                \n",
    "            else:\n",
    "                print('error', typ)\n",
    "                \n",
    "    if sum(test.isnull().sum()) > 0:\n",
    "        pdb.set_trace()\n",
    "    if sum(train.isnull().sum()) > 0:\n",
    "        pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114321"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test_dummies.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete v1_na\n",
      "delete v4_na\n",
      "delete v6_na\n",
      "delete v7_na\n",
      "delete v9_na\n",
      "delete v11_na\n",
      "delete v13_na\n",
      "delete v15_na\n",
      "delete v17_na\n",
      "delete v18_na\n",
      "delete v19_na\n",
      "delete v20_na\n",
      "delete v26_na\n",
      "delete v27_na\n",
      "delete v28_na\n",
      "delete v29_na\n",
      "delete v32_na\n",
      "delete v33_na\n",
      "delete v35_na\n",
      "delete v37_na\n",
      "delete v39_na\n",
      "delete v41_na\n",
      "delete v42_na\n",
      "delete v43_na\n",
      "delete v44_na\n",
      "delete v45_na\n",
      "delete v48_na\n",
      "delete v49_na\n",
      "delete v53_na\n",
      "delete v55_na\n",
      "delete v57_na\n",
      "delete v58_na\n",
      "delete v59_na\n",
      "delete v60_na\n",
      "delete v61_na\n",
      "delete v64_na\n",
      "delete v65_na\n",
      "delete v67_na\n",
      "delete v68_na\n",
      "delete v73_na\n",
      "delete v76_na\n",
      "delete v77_na\n",
      "delete v80_na\n",
      "delete v83_na\n",
      "delete v84_na\n",
      "delete v86_na\n",
      "delete v88_na\n",
      "delete v90_na\n",
      "delete v92_na\n",
      "delete v93_na\n",
      "delete v94_na\n",
      "delete v95_na\n",
      "delete v96_na\n",
      "delete v97_na\n",
      "delete v99_na\n",
      "delete v100_na\n",
      "delete v101_na\n",
      "delete v103_na\n",
      "delete v104_na\n",
      "delete v106_na\n",
      "delete v111_na\n",
      "delete v116_na\n",
      "delete v118_na\n",
      "delete v120_na\n",
      "delete v121_na\n",
      "delete v122_na\n",
      "delete v126_na\n",
      "delete v127_na\n",
      "delete v130_na\n"
     ]
    }
   ],
   "source": [
    "for elt in train.columns:\n",
    "    if (elt[-2:] == 'na') & (elt != 'v2_na'):\n",
    "        # print str(elt)\n",
    "        dist = metrics.pairwise_distances(train.v2_na.reshape(1, -1), train[elt].reshape(1, -1))\n",
    "        if dist < 8:\n",
    "            print \"delete \" + str(elt)\n",
    "            del train[elt],test[elt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.concat([train, train_dummies, target], axis = 1)\n",
    "test = pd.concat([test, test_dummies], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114321, 787)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for elt in list(set(train.columns) - set(test.columns)):\n",
    "    del train[elt]\n",
    "for elt in list(set(test.columns) - set(train.columns)):\n",
    "    del test[elt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114321, 766)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114393, 766)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>...</th>\n",
       "      <th>v129_1.0</th>\n",
       "      <th>v129_2.0</th>\n",
       "      <th>v129_3.0</th>\n",
       "      <th>v129_4.0</th>\n",
       "      <th>v129_5.0</th>\n",
       "      <th>v129_6.0</th>\n",
       "      <th>v129_7.0</th>\n",
       "      <th>v129_8.0</th>\n",
       "      <th>v129_11.0</th>\n",
       "      <th>v129_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.204694</td>\n",
       "      <td>2.372808</td>\n",
       "      <td>1.778510</td>\n",
       "      <td>2.294076</td>\n",
       "      <td>1.525899</td>\n",
       "      <td>1.644205</td>\n",
       "      <td>0.699597</td>\n",
       "      <td>2.484907</td>\n",
       "      <td>0.917602</td>\n",
       "      <td>2.914203</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.415134</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.458994</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.197827</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.079727</td>\n",
       "      <td>1.989254</td>\n",
       "      <td>1.858010</td>\n",
       "      <td>1.991451</td>\n",
       "      <td>1.788352</td>\n",
       "      <td>1.779783</td>\n",
       "      <td>0.702922</td>\n",
       "      <td>2.685577</td>\n",
       "      <td>1.017353</td>\n",
       "      <td>2.818762</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.028696</td>\n",
       "      <td>2.332606</td>\n",
       "      <td>1.828723</td>\n",
       "      <td>2.612085</td>\n",
       "      <td>1.410426</td>\n",
       "      <td>1.383177</td>\n",
       "      <td>0.775624</td>\n",
       "      <td>2.394755</td>\n",
       "      <td>2.145074</td>\n",
       "      <td>2.909492</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.115249</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v4        v5        v6        v7        v8  \\\n",
       "0  1.204694  2.372808  1.778510  2.294076  1.525899  1.644205  0.699597   \n",
       "1  0.000000  0.000000  0.000000  2.415134  0.000000  0.000000  1.458994   \n",
       "2  1.079727  1.989254  1.858010  1.991451  1.788352  1.779783  0.702922   \n",
       "3  1.028696  2.332606  1.828723  2.612085  1.410426  1.383177  0.775624   \n",
       "4  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "\n",
       "         v9       v10       v11    ...     v129_1.0  v129_2.0  v129_3.0  \\\n",
       "0  2.484907  0.917602  2.914203    ...            0         0         0   \n",
       "1  0.000000  1.197827  0.000000    ...            0         0         0   \n",
       "2  2.685577  1.017353  2.818762    ...            0         1         0   \n",
       "3  2.394755  2.145074  2.909492    ...            1         0         0   \n",
       "4  0.000000  1.115249  0.000000    ...            0         0         0   \n",
       "\n",
       "   v129_4.0  v129_5.0  v129_6.0  v129_7.0  v129_8.0  v129_11.0  v129_nan  \n",
       "0         0         0         0         0         0          0         0  \n",
       "1         0         0         0         0         0          0         0  \n",
       "2         0         0         0         0         0          0         0  \n",
       "3         0         0         0         0         0          0         0  \n",
       "4         0         0         0         0         0          0         0  \n",
       "\n",
       "[5 rows x 766 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>v4</th>\n",
       "      <th>v5</th>\n",
       "      <th>v6</th>\n",
       "      <th>v7</th>\n",
       "      <th>v8</th>\n",
       "      <th>v9</th>\n",
       "      <th>v10</th>\n",
       "      <th>v11</th>\n",
       "      <th>...</th>\n",
       "      <th>v129_1.0</th>\n",
       "      <th>v129_2.0</th>\n",
       "      <th>v129_3.0</th>\n",
       "      <th>v129_4.0</th>\n",
       "      <th>v129_5.0</th>\n",
       "      <th>v129_6.0</th>\n",
       "      <th>v129_7.0</th>\n",
       "      <th>v129_8.0</th>\n",
       "      <th>v129_11.0</th>\n",
       "      <th>v129_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.216533</td>\n",
       "      <td>2.592351</td>\n",
       "      <td>1.824675</td>\n",
       "      <td>2.149084</td>\n",
       "      <td>1.406534</td>\n",
       "      <td>1.330884</td>\n",
       "      <td>0.698679</td>\n",
       "      <td>2.444415</td>\n",
       "      <td>1.197827</td>\n",
       "      <td>2.936611</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.191200</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.693147</td>\n",
       "      <td>2.322538</td>\n",
       "      <td>1.878605</td>\n",
       "      <td>2.145943</td>\n",
       "      <td>1.269323</td>\n",
       "      <td>1.496836</td>\n",
       "      <td>0.696723</td>\n",
       "      <td>2.212973</td>\n",
       "      <td>1.274103</td>\n",
       "      <td>2.841582</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.539417</td>\n",
       "      <td>1.617652</td>\n",
       "      <td>1.296702</td>\n",
       "      <td>2.465816</td>\n",
       "      <td>1.405689</td>\n",
       "      <td>1.232809</td>\n",
       "      <td>1.179318</td>\n",
       "      <td>2.298536</td>\n",
       "      <td>1.274103</td>\n",
       "      <td>2.798765</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.179523</td>\n",
       "      <td>2.586511</td>\n",
       "      <td>1.892869</td>\n",
       "      <td>2.353327</td>\n",
       "      <td>1.459192</td>\n",
       "      <td>1.706593</td>\n",
       "      <td>0.729606</td>\n",
       "      <td>2.263106</td>\n",
       "      <td>1.115249</td>\n",
       "      <td>2.867330</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 766 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         v1        v2        v4        v5        v6        v7        v8  \\\n",
       "0  1.216533  2.592351  1.824675  2.149084  1.406534  1.330884  0.698679   \n",
       "1  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.693147  2.322538  1.878605  2.145943  1.269323  1.496836  0.696723   \n",
       "3  1.539417  1.617652  1.296702  2.465816  1.405689  1.232809  1.179318   \n",
       "4  1.179523  2.586511  1.892869  2.353327  1.459192  1.706593  0.729606   \n",
       "\n",
       "         v9       v10       v11    ...     v129_1.0  v129_2.0  v129_3.0  \\\n",
       "0  2.444415  1.197827  2.936611    ...            0         0         0   \n",
       "1  0.000000  1.191200  0.000000    ...            0         0         0   \n",
       "2  2.212973  1.274103  2.841582    ...            0         0         0   \n",
       "3  2.298536  1.274103  2.798765    ...            0         0         0   \n",
       "4  2.263106  1.115249  2.867330    ...            0         0         0   \n",
       "\n",
       "   v129_4.0  v129_5.0  v129_6.0  v129_7.0  v129_8.0  v129_11.0  v129_nan  \n",
       "0         0         0         0         0         0          0         0  \n",
       "1         0         0         0         0         0          0         0  \n",
       "2         0         0         0         0         0          0         0  \n",
       "3         0         0         0         0         0          0         0  \n",
       "4         0         0         0         0         0          0         0  \n",
       "\n",
       "[5 rows x 766 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'v122', u'v123', u'v124', u'v126', u'v127', u'v128', u'v130', u'v131',\n",
       "       u'number_NAN', u'v2_na',\n",
       "       ...\n",
       "       u'v129_1.0', u'v129_2.0', u'v129_3.0', u'v129_4.0', u'v129_5.0',\n",
       "       u'v129_6.0', u'v129_7.0', u'v129_8.0', u'v129_11.0', u'v129_nan'],\n",
       "      dtype='object', length=666)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([ 22575,  22576,  22580, ..., 114318, 114319, 114320]),\n",
       "  array([    0,     1,     2, ..., 22961, 22962, 22964])),\n",
       " (array([     0,      1,      2, ..., 114318, 114319, 114320]),\n",
       "  array([22575, 22576, 22580, ..., 45728, 45729, 45731])),\n",
       " (array([     0,      1,      2, ..., 114318, 114319, 114320]),\n",
       "  array([45724, 45726, 45730, ..., 68871, 68875, 68882])),\n",
       " (array([     0,      1,      2, ..., 114318, 114319, 114320]),\n",
       "  array([68503, 68504, 68505, ..., 91556, 91559, 91560])),\n",
       " (array([    0,     1,     2, ..., 91556, 91559, 91560]),\n",
       "  array([ 91424,  91425,  91426, ..., 114318, 114319, 114320]))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_folds = 5\n",
    "skf = list(StratifiedKFold(target, n_folds))\n",
    "skf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91456"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skf[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22865"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(skf[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n    ensemble.RandomForestClassifier(bootstrap=False, class_weight='auto', criterion='entropy',\\n            max_depth = None, max_features='sqrt', max_leaf_nodes=None,\\n            min_samples_leaf=1, min_samples_split=2,\\n            min_weight_fraction_leaf=0.0, n_estimators = 3000, n_jobs=-1,\\n            oob_score=False, random_state=rnd, verbose=0,\\n            warm_start=False),\\n    ensemble.ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\\n           max_depth = None, max_features='sqrt', max_leaf_nodes=None,\\n           min_samples_leaf=1, min_samples_split=2,\\n           min_weight_fraction_leaf=1e-5, n_estimators = 3000, n_jobs=-1,\\n           oob_score=False, random_state=rnd, verbose=0, warm_start=False)\\n    \""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnd = 57\n",
    "xgboost_params_1 = { \n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"eta\": 0.01, # 0.06, #0.01,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.4,\n",
    "        \"max_depth\": 12,\n",
    "        'silent': 0,\n",
    "        'gamma': 0.1,\n",
    "        'verbose_eval': True,\n",
    "        'seed': rnd\n",
    "    }\n",
    "\n",
    "xgboost_params_2 = { \n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"eta\": 0.01, # 0.06, #0.01,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.2,\n",
    "        \"max_depth\": 10,\n",
    "        'silent': 0,\n",
    "        'gamma': 0.1,\n",
    "        'verbose_eval': True,\n",
    "        'seed': rnd\n",
    "    }\n",
    "\n",
    "xgboost_params_3 = { \n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"eta\": 0.01, # 0.06, #0.01,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"max_depth\": 12,\n",
    "        'silent': 0,\n",
    "        'gamma': 0.1,\n",
    "        'verbose_eval': True\n",
    "    }\n",
    "\n",
    "xgboost_params_4 = { \n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"booster\": \"gbtree\",\n",
    "        \"eval_metric\": \"logloss\",\n",
    "        \"eta\": 0.01, # 0.06, #0.01,\n",
    "        \"min_child_weight\": 1,\n",
    "        \"subsample\": 0.9,\n",
    "        \"colsample_bytree\": 0.5,\n",
    "        \"max_depth\": 14,\n",
    "        'silent': 0,\n",
    "        'gamma': 0.1,\n",
    "        'verbose_eval': True\n",
    "    }\n",
    "\n",
    "\n",
    "clfs=[\n",
    "    'xgboost-1',\n",
    "    'xgboost-2',\n",
    "    ensemble.RandomForestClassifier(bootstrap=False, class_weight='auto', criterion='entropy',\n",
    "            max_depth = None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_samples_leaf = 4, min_samples_split = 4,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators = 3000, n_jobs=-1,\n",
    "            oob_score=False, random_state=rnd, verbose=0,\n",
    "            warm_start=False),\n",
    "    ensemble.ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
    "           max_depth = None, max_features='sqrt', max_leaf_nodes=None,\n",
    "           min_samples_leaf = 4, min_samples_split = 4,\n",
    "           min_weight_fraction_leaf=1e-5, n_estimators = 3000, n_jobs=-1,\n",
    "           oob_score=False, random_state=rnd, verbose=0, warm_start=False),\n",
    "    ensemble.GradientBoostingClassifier(init = None, learning_rate = 0.1, loss = 'deviance',\n",
    "              max_depth = 30, max_features = None, max_leaf_nodes = None,\n",
    "              min_samples_leaf = 4, min_samples_split = 4,\n",
    "              min_weight_fraction_leaf = 0.0, n_estimators = 100,\n",
    "              random_state = rnd, subsample = 0.8, verbose = 0,\n",
    "              warm_start = False)\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# clfs = ['xgboost-1', 'xgboost-2', 'xgboost-3', 'xgboost-4']\n",
    "\n",
    "'''\n",
    "clfs = ['xgboost-1']\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "clfs = [\n",
    "    ensemble.GradientBoostingClassifier(init = None, learning_rate = 0.1, loss = 'deviance',\n",
    "              max_depth = 15, max_features = None, max_leaf_nodes = None,\n",
    "              min_samples_leaf = 1, min_samples_split = 3,\n",
    "              min_weight_fraction_leaf = 0.0, n_estimators = 100,\n",
    "              random_state = rnd, subsample = 0.8, verbose = 0,\n",
    "              warm_start = False)\n",
    "]\n",
    "\n",
    "\n",
    "clfs=[\n",
    "    ensemble.ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
    "           max_depth = None, max_features='sqrt', max_leaf_nodes=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=1e-5, n_estimators = 1000, n_jobs=-1,\n",
    "           oob_score=False, random_state=rnd, verbose=0, warm_start=False)\n",
    "]\n",
    "'''\n",
    "\n",
    "'''\n",
    "    ensemble.RandomForestClassifier(bootstrap=False, class_weight='auto', criterion='entropy',\n",
    "            max_depth = None, max_features='sqrt', max_leaf_nodes=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators = 3000, n_jobs=-1,\n",
    "            oob_score=False, random_state=rnd, verbose=0,\n",
    "            warm_start=False),\n",
    "    ensemble.ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
    "           max_depth = None, max_features='sqrt', max_leaf_nodes=None,\n",
    "           min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=1e-5, n_estimators = 3000, n_jobs=-1,\n",
    "           oob_score=False, random_state=rnd, verbose=0, warm_start=False)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xgboost-1',\n",
       " 'xgboost-2',\n",
       " RandomForestClassifier(bootstrap=False, class_weight='auto',\n",
       "             criterion='entropy', max_depth=None, max_features='sqrt',\n",
       "             max_leaf_nodes=None, min_samples_leaf=4, min_samples_split=4,\n",
       "             min_weight_fraction_leaf=0.0, n_estimators=3000, n_jobs=-1,\n",
       "             oob_score=False, random_state=57, verbose=0, warm_start=False),\n",
       " ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='entropy',\n",
       "            max_depth=None, max_features='sqrt', max_leaf_nodes=None,\n",
       "            min_samples_leaf=4, min_samples_split=4,\n",
       "            min_weight_fraction_leaf=1e-05, n_estimators=3000, n_jobs=-1,\n",
       "            oob_score=False, random_state=57, verbose=0, warm_start=False),\n",
       " GradientBoostingClassifier(init=None, learning_rate=0.1, loss='deviance',\n",
       "               max_depth=30, max_features=None, max_leaf_nodes=None,\n",
       "               min_samples_leaf=4, min_samples_split=4,\n",
       "               min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "               presort='auto', random_state=57, subsample=0.8, verbose=0,\n",
       "               warm_start=False)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_submit = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_rounds = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<enumerate at 0x10852e410>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enumerate(skf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until eval error hasn't decreased in 100 rounds.\n",
      "[0]\ttrain-logloss:0.689227\teval-logloss:0.689665\n",
      "[1]\ttrain-logloss:0.684972\teval-logloss:0.686033\n",
      "[2]\ttrain-logloss:0.680827\teval-logloss:0.682395\n",
      "[3]\ttrain-logloss:0.676602\teval-logloss:0.678798\n",
      "[4]\ttrain-logloss:0.673005\teval-logloss:0.675600\n",
      "[5]\ttrain-logloss:0.669123\teval-logloss:0.672325\n",
      "[6]\ttrain-logloss:0.665736\teval-logloss:0.669282\n",
      "[7]\ttrain-logloss:0.662125\teval-logloss:0.666251\n",
      "[8]\ttrain-logloss:0.658202\teval-logloss:0.662992\n",
      "[9]\ttrain-logloss:0.654392\teval-logloss:0.659756\n",
      "[10]\ttrain-logloss:0.651287\teval-logloss:0.657056\n",
      "[11]\ttrain-logloss:0.647581\teval-logloss:0.653981\n",
      "[12]\ttrain-logloss:0.644678\teval-logloss:0.651400\n",
      "[13]\ttrain-logloss:0.641221\teval-logloss:0.648473\n",
      "[14]\ttrain-logloss:0.638195\teval-logloss:0.645905\n",
      "[15]\ttrain-logloss:0.635273\teval-logloss:0.643279\n",
      "[16]\ttrain-logloss:0.631908\teval-logloss:0.640487\n",
      "[17]\ttrain-logloss:0.628738\teval-logloss:0.637909\n",
      "[18]\ttrain-logloss:0.625507\teval-logloss:0.635221\n",
      "[19]\ttrain-logloss:0.622299\teval-logloss:0.632592\n",
      "[20]\ttrain-logloss:0.619515\teval-logloss:0.630267\n",
      "[21]\ttrain-logloss:0.616649\teval-logloss:0.627910\n",
      "[22]\ttrain-logloss:0.613557\teval-logloss:0.625310\n",
      "[23]\ttrain-logloss:0.610439\teval-logloss:0.622753\n",
      "[24]\ttrain-logloss:0.607892\teval-logloss:0.620541\n",
      "[25]\ttrain-logloss:0.605449\teval-logloss:0.618421\n",
      "[26]\ttrain-logloss:0.602425\teval-logloss:0.615984\n",
      "[27]\ttrain-logloss:0.599649\teval-logloss:0.613766\n",
      "[28]\ttrain-logloss:0.597188\teval-logloss:0.611754\n",
      "[29]\ttrain-logloss:0.594494\teval-logloss:0.609580\n",
      "[30]\ttrain-logloss:0.591671\teval-logloss:0.607339\n",
      "[31]\ttrain-logloss:0.589267\teval-logloss:0.605355\n",
      "[32]\ttrain-logloss:0.586528\teval-logloss:0.603121\n",
      "[33]\ttrain-logloss:0.584256\teval-logloss:0.601254\n",
      "[34]\ttrain-logloss:0.582104\teval-logloss:0.599401\n",
      "[35]\ttrain-logloss:0.579471\teval-logloss:0.597340\n",
      "[36]\ttrain-logloss:0.576865\teval-logloss:0.595258\n",
      "[37]\ttrain-logloss:0.574483\teval-logloss:0.593410\n",
      "[38]\ttrain-logloss:0.571965\teval-logloss:0.591440\n",
      "[39]\ttrain-logloss:0.569696\teval-logloss:0.589709\n",
      "[40]\ttrain-logloss:0.567322\teval-logloss:0.587829\n",
      "[41]\ttrain-logloss:0.565098\teval-logloss:0.586103\n",
      "[42]\ttrain-logloss:0.563141\teval-logloss:0.584461\n",
      "[43]\ttrain-logloss:0.561185\teval-logloss:0.582882\n",
      "[44]\ttrain-logloss:0.559110\teval-logloss:0.581270\n",
      "[45]\ttrain-logloss:0.557246\teval-logloss:0.579779\n",
      "[46]\ttrain-logloss:0.554909\teval-logloss:0.577965\n",
      "[47]\ttrain-logloss:0.553094\teval-logloss:0.576468\n",
      "[48]\ttrain-logloss:0.551275\teval-logloss:0.575055\n",
      "[49]\ttrain-logloss:0.549321\teval-logloss:0.573589\n",
      "[50]\ttrain-logloss:0.547294\teval-logloss:0.572125\n",
      "[51]\ttrain-logloss:0.545119\teval-logloss:0.570441\n",
      "[52]\ttrain-logloss:0.543023\teval-logloss:0.568795\n",
      "[53]\ttrain-logloss:0.541432\teval-logloss:0.567487\n",
      "[54]\ttrain-logloss:0.539315\teval-logloss:0.565949\n",
      "[55]\ttrain-logloss:0.537307\teval-logloss:0.564461\n",
      "[56]\ttrain-logloss:0.535289\teval-logloss:0.562944\n",
      "[57]\ttrain-logloss:0.533420\teval-logloss:0.561561\n",
      "[58]\ttrain-logloss:0.531428\teval-logloss:0.560039\n",
      "[59]\ttrain-logloss:0.529547\teval-logloss:0.558591\n",
      "[60]\ttrain-logloss:0.528045\teval-logloss:0.557429\n",
      "[61]\ttrain-logloss:0.526143\teval-logloss:0.555996\n",
      "[62]\ttrain-logloss:0.524254\teval-logloss:0.554586\n",
      "[63]\ttrain-logloss:0.522324\teval-logloss:0.553265\n",
      "[64]\ttrain-logloss:0.520438\teval-logloss:0.551909\n",
      "[65]\ttrain-logloss:0.518593\teval-logloss:0.550624\n",
      "[66]\ttrain-logloss:0.516742\teval-logloss:0.549273\n",
      "[67]\ttrain-logloss:0.514970\teval-logloss:0.547957\n",
      "[68]\ttrain-logloss:0.513300\teval-logloss:0.546817\n",
      "[69]\ttrain-logloss:0.511603\teval-logloss:0.545638\n",
      "[70]\ttrain-logloss:0.510155\teval-logloss:0.544640\n",
      "[71]\ttrain-logloss:0.508434\teval-logloss:0.543424\n",
      "[72]\ttrain-logloss:0.506871\teval-logloss:0.542368\n",
      "[73]\ttrain-logloss:0.505192\teval-logloss:0.541208\n",
      "[74]\ttrain-logloss:0.503644\teval-logloss:0.540197\n",
      "[75]\ttrain-logloss:0.502009\teval-logloss:0.539042\n",
      "[76]\ttrain-logloss:0.500589\teval-logloss:0.538044\n",
      "[77]\ttrain-logloss:0.499054\teval-logloss:0.537002\n",
      "[78]\ttrain-logloss:0.497470\teval-logloss:0.535903\n",
      "[79]\ttrain-logloss:0.495863\teval-logloss:0.534818\n",
      "[80]\ttrain-logloss:0.494376\teval-logloss:0.533848\n",
      "[81]\ttrain-logloss:0.492770\teval-logloss:0.532821\n",
      "[82]\ttrain-logloss:0.491528\teval-logloss:0.532007\n",
      "[83]\ttrain-logloss:0.490016\teval-logloss:0.530962\n",
      "[84]\ttrain-logloss:0.488664\teval-logloss:0.530124\n",
      "[85]\ttrain-logloss:0.487476\teval-logloss:0.529296\n",
      "[86]\ttrain-logloss:0.486344\teval-logloss:0.528501\n",
      "[87]\ttrain-logloss:0.485192\teval-logloss:0.527676\n",
      "[88]\ttrain-logloss:0.484073\teval-logloss:0.526851\n",
      "[89]\ttrain-logloss:0.482680\teval-logloss:0.525917\n",
      "[90]\ttrain-logloss:0.481386\teval-logloss:0.525110\n",
      "[91]\ttrain-logloss:0.479971\teval-logloss:0.524159\n",
      "[92]\ttrain-logloss:0.478640\teval-logloss:0.523347\n",
      "[93]\ttrain-logloss:0.477547\teval-logloss:0.522585\n",
      "[94]\ttrain-logloss:0.476172\teval-logloss:0.521679\n",
      "[95]\ttrain-logloss:0.474783\teval-logloss:0.520829\n",
      "[96]\ttrain-logloss:0.473395\teval-logloss:0.519946\n",
      "[97]\ttrain-logloss:0.472150\teval-logloss:0.519150\n",
      "[98]\ttrain-logloss:0.470950\teval-logloss:0.518386\n",
      "[99]\ttrain-logloss:0.469863\teval-logloss:0.517703\n",
      "[100]\ttrain-logloss:0.468806\teval-logloss:0.517034\n",
      "[101]\ttrain-logloss:0.467650\teval-logloss:0.516364\n",
      "[102]\ttrain-logloss:0.466426\teval-logloss:0.515623\n",
      "[103]\ttrain-logloss:0.465168\teval-logloss:0.514836\n",
      "[104]\ttrain-logloss:0.464098\teval-logloss:0.514232\n",
      "[105]\ttrain-logloss:0.463158\teval-logloss:0.513664\n",
      "[106]\ttrain-logloss:0.461914\teval-logloss:0.512901\n",
      "[107]\ttrain-logloss:0.460892\teval-logloss:0.512194\n",
      "[108]\ttrain-logloss:0.459983\teval-logloss:0.511601\n",
      "[109]\ttrain-logloss:0.459086\teval-logloss:0.511014\n",
      "[110]\ttrain-logloss:0.457992\teval-logloss:0.510345\n",
      "[111]\ttrain-logloss:0.456849\teval-logloss:0.509692\n",
      "[112]\ttrain-logloss:0.456031\teval-logloss:0.509171\n",
      "[113]\ttrain-logloss:0.454831\teval-logloss:0.508516\n",
      "[114]\ttrain-logloss:0.453944\teval-logloss:0.507984\n",
      "[115]\ttrain-logloss:0.452778\teval-logloss:0.507297\n",
      "[116]\ttrain-logloss:0.451911\teval-logloss:0.506796\n",
      "[117]\ttrain-logloss:0.450954\teval-logloss:0.506266\n",
      "[118]\ttrain-logloss:0.450159\teval-logloss:0.505765\n",
      "[119]\ttrain-logloss:0.449298\teval-logloss:0.505242\n",
      "[120]\ttrain-logloss:0.448243\teval-logloss:0.504697\n",
      "[121]\ttrain-logloss:0.447245\teval-logloss:0.504169\n",
      "[122]\ttrain-logloss:0.446173\teval-logloss:0.503547\n",
      "[123]\ttrain-logloss:0.445318\teval-logloss:0.503074\n",
      "[124]\ttrain-logloss:0.444604\teval-logloss:0.502619\n",
      "[125]\ttrain-logloss:0.443590\teval-logloss:0.501994\n",
      "[126]\ttrain-logloss:0.442485\teval-logloss:0.501390\n",
      "[127]\ttrain-logloss:0.441409\teval-logloss:0.500801\n",
      "[128]\ttrain-logloss:0.440408\teval-logloss:0.500302\n",
      "[129]\ttrain-logloss:0.439383\teval-logloss:0.499721\n",
      "[130]\ttrain-logloss:0.438652\teval-logloss:0.499273\n",
      "[131]\ttrain-logloss:0.437749\teval-logloss:0.498794\n",
      "[132]\ttrain-logloss:0.436829\teval-logloss:0.498323\n",
      "[133]\ttrain-logloss:0.436019\teval-logloss:0.497903\n",
      "[134]\ttrain-logloss:0.435044\teval-logloss:0.497351\n",
      "[135]\ttrain-logloss:0.434089\teval-logloss:0.496847\n",
      "[136]\ttrain-logloss:0.433358\teval-logloss:0.496446\n",
      "[137]\ttrain-logloss:0.432544\teval-logloss:0.496046\n",
      "[138]\ttrain-logloss:0.431576\teval-logloss:0.495550\n",
      "[139]\ttrain-logloss:0.430802\teval-logloss:0.495175\n",
      "[140]\ttrain-logloss:0.429846\teval-logloss:0.494672\n",
      "[141]\ttrain-logloss:0.428988\teval-logloss:0.494227\n",
      "[142]\ttrain-logloss:0.428091\teval-logloss:0.493779\n",
      "[143]\ttrain-logloss:0.427298\teval-logloss:0.493375\n",
      "[144]\ttrain-logloss:0.426648\teval-logloss:0.493030\n",
      "[145]\ttrain-logloss:0.425821\teval-logloss:0.492635\n",
      "[146]\ttrain-logloss:0.424973\teval-logloss:0.492234\n",
      "[147]\ttrain-logloss:0.424060\teval-logloss:0.491873\n",
      "[148]\ttrain-logloss:0.423161\teval-logloss:0.491501\n",
      "[149]\ttrain-logloss:0.422549\teval-logloss:0.491179\n",
      "[150]\ttrain-logloss:0.421862\teval-logloss:0.490822\n",
      "[151]\ttrain-logloss:0.421166\teval-logloss:0.490529\n",
      "[152]\ttrain-logloss:0.420431\teval-logloss:0.490215\n",
      "[153]\ttrain-logloss:0.419525\teval-logloss:0.489792\n",
      "[154]\ttrain-logloss:0.418942\teval-logloss:0.489506\n",
      "[155]\ttrain-logloss:0.418129\teval-logloss:0.489143\n",
      "[156]\ttrain-logloss:0.417270\teval-logloss:0.488711\n",
      "[157]\ttrain-logloss:0.416488\teval-logloss:0.488385\n",
      "[158]\ttrain-logloss:0.415675\teval-logloss:0.488005\n",
      "[159]\ttrain-logloss:0.414984\teval-logloss:0.487715\n",
      "[160]\ttrain-logloss:0.414284\teval-logloss:0.487409\n",
      "[161]\ttrain-logloss:0.413731\teval-logloss:0.487164\n",
      "[162]\ttrain-logloss:0.413061\teval-logloss:0.486836\n",
      "[163]\ttrain-logloss:0.412431\teval-logloss:0.486541\n",
      "[164]\ttrain-logloss:0.411644\teval-logloss:0.486157\n",
      "[165]\ttrain-logloss:0.410975\teval-logloss:0.485892\n",
      "[166]\ttrain-logloss:0.410327\teval-logloss:0.485604\n",
      "[167]\ttrain-logloss:0.409541\teval-logloss:0.485221\n",
      "[168]\ttrain-logloss:0.408768\teval-logloss:0.484873\n",
      "[169]\ttrain-logloss:0.408166\teval-logloss:0.484633\n",
      "[170]\ttrain-logloss:0.407663\teval-logloss:0.484387\n",
      "[171]\ttrain-logloss:0.406877\teval-logloss:0.484100\n",
      "[172]\ttrain-logloss:0.406021\teval-logloss:0.483783\n",
      "[173]\ttrain-logloss:0.405401\teval-logloss:0.483503\n",
      "[174]\ttrain-logloss:0.404910\teval-logloss:0.483283\n",
      "[175]\ttrain-logloss:0.404270\teval-logloss:0.483037\n",
      "[176]\ttrain-logloss:0.403649\teval-logloss:0.482776\n",
      "[177]\ttrain-logloss:0.403242\teval-logloss:0.482581\n",
      "[178]\ttrain-logloss:0.402612\teval-logloss:0.482333\n",
      "[179]\ttrain-logloss:0.401928\teval-logloss:0.482090\n",
      "[180]\ttrain-logloss:0.401223\teval-logloss:0.481759\n",
      "[181]\ttrain-logloss:0.400491\teval-logloss:0.481439\n",
      "[182]\ttrain-logloss:0.399818\teval-logloss:0.481200\n",
      "[183]\ttrain-logloss:0.399072\teval-logloss:0.480899\n",
      "[184]\ttrain-logloss:0.398428\teval-logloss:0.480682\n",
      "[185]\ttrain-logloss:0.397707\teval-logloss:0.480395\n",
      "[186]\ttrain-logloss:0.397056\teval-logloss:0.480086\n",
      "[187]\ttrain-logloss:0.396410\teval-logloss:0.479854\n",
      "[188]\ttrain-logloss:0.395745\teval-logloss:0.479567\n",
      "[189]\ttrain-logloss:0.395054\teval-logloss:0.479291\n",
      "[190]\ttrain-logloss:0.394511\teval-logloss:0.479114\n",
      "[191]\ttrain-logloss:0.393947\teval-logloss:0.478874\n",
      "[192]\ttrain-logloss:0.393309\teval-logloss:0.478601\n",
      "[193]\ttrain-logloss:0.392746\teval-logloss:0.478438\n",
      "[194]\ttrain-logloss:0.392168\teval-logloss:0.478241\n",
      "[195]\ttrain-logloss:0.391590\teval-logloss:0.478059\n",
      "[196]\ttrain-logloss:0.391030\teval-logloss:0.477874\n",
      "[197]\ttrain-logloss:0.390529\teval-logloss:0.477725\n",
      "[198]\ttrain-logloss:0.389921\teval-logloss:0.477474\n",
      "[199]\ttrain-logloss:0.389375\teval-logloss:0.477246\n",
      "[200]\ttrain-logloss:0.388785\teval-logloss:0.477016\n",
      "[201]\ttrain-logloss:0.388260\teval-logloss:0.476823\n",
      "[202]\ttrain-logloss:0.387819\teval-logloss:0.476673\n",
      "[203]\ttrain-logloss:0.387361\teval-logloss:0.476493\n",
      "[204]\ttrain-logloss:0.386920\teval-logloss:0.476350\n",
      "[205]\ttrain-logloss:0.386375\teval-logloss:0.476151\n",
      "[206]\ttrain-logloss:0.385873\teval-logloss:0.475995\n",
      "[207]\ttrain-logloss:0.385249\teval-logloss:0.475766\n",
      "[208]\ttrain-logloss:0.384873\teval-logloss:0.475622\n",
      "[209]\ttrain-logloss:0.384228\teval-logloss:0.475398\n",
      "[210]\ttrain-logloss:0.383606\teval-logloss:0.475134\n",
      "[211]\ttrain-logloss:0.383039\teval-logloss:0.474908\n",
      "[212]\ttrain-logloss:0.382451\teval-logloss:0.474684\n",
      "[213]\ttrain-logloss:0.381862\teval-logloss:0.474453\n",
      "[214]\ttrain-logloss:0.381335\teval-logloss:0.474298\n",
      "[215]\ttrain-logloss:0.380957\teval-logloss:0.474147\n",
      "[216]\ttrain-logloss:0.380505\teval-logloss:0.473968\n",
      "[217]\ttrain-logloss:0.379985\teval-logloss:0.473807\n",
      "[218]\ttrain-logloss:0.379417\teval-logloss:0.473673\n",
      "[219]\ttrain-logloss:0.378762\teval-logloss:0.473460\n",
      "[220]\ttrain-logloss:0.378171\teval-logloss:0.473265\n",
      "[221]\ttrain-logloss:0.377577\teval-logloss:0.473098\n",
      "[222]\ttrain-logloss:0.377058\teval-logloss:0.472938\n",
      "[223]\ttrain-logloss:0.376590\teval-logloss:0.472783\n",
      "[224]\ttrain-logloss:0.376238\teval-logloss:0.472673\n",
      "[225]\ttrain-logloss:0.375773\teval-logloss:0.472521\n",
      "[226]\ttrain-logloss:0.375275\teval-logloss:0.472396\n",
      "[227]\ttrain-logloss:0.374711\teval-logloss:0.472221\n",
      "[228]\ttrain-logloss:0.374176\teval-logloss:0.472098\n",
      "[229]\ttrain-logloss:0.373819\teval-logloss:0.471989\n",
      "[230]\ttrain-logloss:0.373408\teval-logloss:0.471857\n",
      "[231]\ttrain-logloss:0.372939\teval-logloss:0.471683\n",
      "[232]\ttrain-logloss:0.372456\teval-logloss:0.471566\n",
      "[233]\ttrain-logloss:0.371986\teval-logloss:0.471419\n",
      "[234]\ttrain-logloss:0.371482\teval-logloss:0.471266\n",
      "[235]\ttrain-logloss:0.371126\teval-logloss:0.471170\n",
      "[236]\ttrain-logloss:0.370603\teval-logloss:0.471033\n",
      "[237]\ttrain-logloss:0.370190\teval-logloss:0.470929\n",
      "[238]\ttrain-logloss:0.369689\teval-logloss:0.470775\n",
      "[239]\ttrain-logloss:0.369146\teval-logloss:0.470633\n",
      "[240]\ttrain-logloss:0.368721\teval-logloss:0.470538\n",
      "[241]\ttrain-logloss:0.368233\teval-logloss:0.470397\n",
      "[242]\ttrain-logloss:0.367794\teval-logloss:0.470303\n",
      "[243]\ttrain-logloss:0.367303\teval-logloss:0.470169\n",
      "[244]\ttrain-logloss:0.366953\teval-logloss:0.470073\n",
      "[245]\ttrain-logloss:0.366526\teval-logloss:0.469991\n",
      "[246]\ttrain-logloss:0.365986\teval-logloss:0.469805\n",
      "[247]\ttrain-logloss:0.365642\teval-logloss:0.469732\n",
      "[248]\ttrain-logloss:0.365214\teval-logloss:0.469648\n",
      "[249]\ttrain-logloss:0.364875\teval-logloss:0.469571\n",
      "[250]\ttrain-logloss:0.364357\teval-logloss:0.469415\n",
      "[251]\ttrain-logloss:0.363871\teval-logloss:0.469308\n",
      "[252]\ttrain-logloss:0.363445\teval-logloss:0.469152\n",
      "[253]\ttrain-logloss:0.362957\teval-logloss:0.469009\n",
      "[254]\ttrain-logloss:0.362557\teval-logloss:0.468919\n",
      "[255]\ttrain-logloss:0.362232\teval-logloss:0.468824\n",
      "[256]\ttrain-logloss:0.361857\teval-logloss:0.468740\n",
      "[257]\ttrain-logloss:0.361409\teval-logloss:0.468633\n",
      "[258]\ttrain-logloss:0.361071\teval-logloss:0.468547\n",
      "[259]\ttrain-logloss:0.360567\teval-logloss:0.468408\n",
      "[260]\ttrain-logloss:0.360052\teval-logloss:0.468259\n",
      "[261]\ttrain-logloss:0.359521\teval-logloss:0.468112\n",
      "[262]\ttrain-logloss:0.359139\teval-logloss:0.468027\n",
      "[263]\ttrain-logloss:0.358740\teval-logloss:0.467877\n",
      "[264]\ttrain-logloss:0.358419\teval-logloss:0.467813\n",
      "[265]\ttrain-logloss:0.358084\teval-logloss:0.467752\n",
      "[266]\ttrain-logloss:0.357677\teval-logloss:0.467672\n",
      "[267]\ttrain-logloss:0.357304\teval-logloss:0.467590\n",
      "[268]\ttrain-logloss:0.357015\teval-logloss:0.467513\n",
      "[269]\ttrain-logloss:0.356576\teval-logloss:0.467425\n",
      "[270]\ttrain-logloss:0.356140\teval-logloss:0.467347\n",
      "[271]\ttrain-logloss:0.355771\teval-logloss:0.467265\n",
      "[272]\ttrain-logloss:0.355383\teval-logloss:0.467147\n",
      "[273]\ttrain-logloss:0.355027\teval-logloss:0.467073\n",
      "[274]\ttrain-logloss:0.354591\teval-logloss:0.466956\n",
      "[275]\ttrain-logloss:0.354170\teval-logloss:0.466836\n",
      "[276]\ttrain-logloss:0.353811\teval-logloss:0.466739\n",
      "[277]\ttrain-logloss:0.353542\teval-logloss:0.466686\n",
      "[278]\ttrain-logloss:0.353274\teval-logloss:0.466624\n",
      "[279]\ttrain-logloss:0.352914\teval-logloss:0.466508\n",
      "[280]\ttrain-logloss:0.352422\teval-logloss:0.466394\n",
      "[281]\ttrain-logloss:0.351932\teval-logloss:0.466280\n",
      "[282]\ttrain-logloss:0.351571\teval-logloss:0.466219\n",
      "[283]\ttrain-logloss:0.351221\teval-logloss:0.466149\n",
      "[284]\ttrain-logloss:0.350812\teval-logloss:0.466084\n",
      "[285]\ttrain-logloss:0.350410\teval-logloss:0.466031\n",
      "[286]\ttrain-logloss:0.350143\teval-logloss:0.465975\n",
      "[287]\ttrain-logloss:0.349822\teval-logloss:0.465909\n",
      "[288]\ttrain-logloss:0.349434\teval-logloss:0.465825\n",
      "[289]\ttrain-logloss:0.349062\teval-logloss:0.465759\n",
      "[290]\ttrain-logloss:0.348733\teval-logloss:0.465630\n",
      "[291]\ttrain-logloss:0.348373\teval-logloss:0.465557\n",
      "[292]\ttrain-logloss:0.347993\teval-logloss:0.465518\n",
      "[293]\ttrain-logloss:0.347649\teval-logloss:0.465458\n",
      "[294]\ttrain-logloss:0.347296\teval-logloss:0.465369\n",
      "[295]\ttrain-logloss:0.346960\teval-logloss:0.465302\n",
      "[296]\ttrain-logloss:0.346653\teval-logloss:0.465223\n",
      "[297]\ttrain-logloss:0.346273\teval-logloss:0.465119\n",
      "[298]\ttrain-logloss:0.345859\teval-logloss:0.465052\n",
      "[299]\ttrain-logloss:0.345631\teval-logloss:0.464999\n",
      "[300]\ttrain-logloss:0.345258\teval-logloss:0.464912\n",
      "[301]\ttrain-logloss:0.344958\teval-logloss:0.464865\n",
      "[302]\ttrain-logloss:0.344628\teval-logloss:0.464824\n",
      "[303]\ttrain-logloss:0.344363\teval-logloss:0.464777\n",
      "[304]\ttrain-logloss:0.344035\teval-logloss:0.464698\n",
      "[305]\ttrain-logloss:0.343747\teval-logloss:0.464655\n",
      "[306]\ttrain-logloss:0.343426\teval-logloss:0.464604\n",
      "[307]\ttrain-logloss:0.343211\teval-logloss:0.464547\n",
      "[308]\ttrain-logloss:0.342890\teval-logloss:0.464458\n",
      "[309]\ttrain-logloss:0.342579\teval-logloss:0.464390\n",
      "[310]\ttrain-logloss:0.342180\teval-logloss:0.464302\n",
      "[311]\ttrain-logloss:0.341890\teval-logloss:0.464257\n",
      "[312]\ttrain-logloss:0.341609\teval-logloss:0.464196\n",
      "[313]\ttrain-logloss:0.341197\teval-logloss:0.464118\n",
      "[314]\ttrain-logloss:0.340862\teval-logloss:0.464052\n",
      "[315]\ttrain-logloss:0.340635\teval-logloss:0.464014\n",
      "[316]\ttrain-logloss:0.340457\teval-logloss:0.463995\n",
      "[317]\ttrain-logloss:0.340124\teval-logloss:0.463905\n",
      "[318]\ttrain-logloss:0.339832\teval-logloss:0.463819\n",
      "[319]\ttrain-logloss:0.339549\teval-logloss:0.463777\n",
      "[320]\ttrain-logloss:0.339411\teval-logloss:0.463731\n",
      "[321]\ttrain-logloss:0.339067\teval-logloss:0.463650\n",
      "[322]\ttrain-logloss:0.338785\teval-logloss:0.463585\n",
      "[323]\ttrain-logloss:0.338601\teval-logloss:0.463563\n",
      "[324]\ttrain-logloss:0.338285\teval-logloss:0.463459\n",
      "[325]\ttrain-logloss:0.338075\teval-logloss:0.463424\n",
      "[326]\ttrain-logloss:0.337712\teval-logloss:0.463352\n",
      "[327]\ttrain-logloss:0.337420\teval-logloss:0.463321\n",
      "[328]\ttrain-logloss:0.337071\teval-logloss:0.463251\n",
      "[329]\ttrain-logloss:0.336852\teval-logloss:0.463237\n",
      "[330]\ttrain-logloss:0.336559\teval-logloss:0.463161\n",
      "[331]\ttrain-logloss:0.336366\teval-logloss:0.463130\n",
      "[332]\ttrain-logloss:0.336111\teval-logloss:0.463104\n",
      "[333]\ttrain-logloss:0.335776\teval-logloss:0.463053\n",
      "[334]\ttrain-logloss:0.335442\teval-logloss:0.463024\n",
      "[335]\ttrain-logloss:0.335224\teval-logloss:0.462980\n",
      "[336]\ttrain-logloss:0.334918\teval-logloss:0.462962\n",
      "[337]\ttrain-logloss:0.334584\teval-logloss:0.462882\n",
      "[338]\ttrain-logloss:0.334298\teval-logloss:0.462845\n",
      "[339]\ttrain-logloss:0.334042\teval-logloss:0.462811\n",
      "[340]\ttrain-logloss:0.333759\teval-logloss:0.462758\n",
      "[341]\ttrain-logloss:0.333557\teval-logloss:0.462737\n",
      "[342]\ttrain-logloss:0.333243\teval-logloss:0.462675\n",
      "[343]\ttrain-logloss:0.333082\teval-logloss:0.462651\n",
      "[344]\ttrain-logloss:0.332797\teval-logloss:0.462613\n",
      "[345]\ttrain-logloss:0.332502\teval-logloss:0.462605\n",
      "[346]\ttrain-logloss:0.332246\teval-logloss:0.462572\n",
      "[347]\ttrain-logloss:0.332003\teval-logloss:0.462521\n",
      "[348]\ttrain-logloss:0.331738\teval-logloss:0.462457\n",
      "[349]\ttrain-logloss:0.331555\teval-logloss:0.462430\n",
      "[350]\ttrain-logloss:0.331396\teval-logloss:0.462403\n",
      "[351]\ttrain-logloss:0.331194\teval-logloss:0.462373\n",
      "[352]\ttrain-logloss:0.330944\teval-logloss:0.462352\n",
      "[353]\ttrain-logloss:0.330710\teval-logloss:0.462313\n",
      "[354]\ttrain-logloss:0.330443\teval-logloss:0.462283\n",
      "[355]\ttrain-logloss:0.330223\teval-logloss:0.462266\n",
      "[356]\ttrain-logloss:0.329873\teval-logloss:0.462212\n",
      "[357]\ttrain-logloss:0.329541\teval-logloss:0.462151\n",
      "[358]\ttrain-logloss:0.329290\teval-logloss:0.462085\n",
      "[359]\ttrain-logloss:0.328988\teval-logloss:0.462040\n",
      "[360]\ttrain-logloss:0.328706\teval-logloss:0.461993\n",
      "[361]\ttrain-logloss:0.328483\teval-logloss:0.461961\n",
      "[362]\ttrain-logloss:0.328339\teval-logloss:0.461932\n",
      "[363]\ttrain-logloss:0.327991\teval-logloss:0.461876\n",
      "[364]\ttrain-logloss:0.327765\teval-logloss:0.461855\n",
      "[365]\ttrain-logloss:0.327531\teval-logloss:0.461847\n",
      "[366]\ttrain-logloss:0.327233\teval-logloss:0.461792\n",
      "[367]\ttrain-logloss:0.326951\teval-logloss:0.461723\n",
      "[368]\ttrain-logloss:0.326619\teval-logloss:0.461662\n",
      "[369]\ttrain-logloss:0.326397\teval-logloss:0.461641\n",
      "[370]\ttrain-logloss:0.326125\teval-logloss:0.461593\n",
      "[371]\ttrain-logloss:0.325787\teval-logloss:0.461573\n",
      "[372]\ttrain-logloss:0.325623\teval-logloss:0.461540\n",
      "[373]\ttrain-logloss:0.325327\teval-logloss:0.461500\n",
      "[374]\ttrain-logloss:0.325101\teval-logloss:0.461493\n",
      "[375]\ttrain-logloss:0.324819\teval-logloss:0.461470\n",
      "[376]\ttrain-logloss:0.324627\teval-logloss:0.461461\n",
      "[377]\ttrain-logloss:0.324338\teval-logloss:0.461414\n",
      "[378]\ttrain-logloss:0.324155\teval-logloss:0.461379\n",
      "[379]\ttrain-logloss:0.323974\teval-logloss:0.461361\n",
      "[380]\ttrain-logloss:0.323725\teval-logloss:0.461341\n",
      "[381]\ttrain-logloss:0.323531\teval-logloss:0.461311\n",
      "[382]\ttrain-logloss:0.323285\teval-logloss:0.461252\n",
      "[383]\ttrain-logloss:0.323049\teval-logloss:0.461241\n",
      "[384]\ttrain-logloss:0.322910\teval-logloss:0.461230\n",
      "[385]\ttrain-logloss:0.322673\teval-logloss:0.461194\n",
      "[386]\ttrain-logloss:0.322558\teval-logloss:0.461166\n",
      "[387]\ttrain-logloss:0.322361\teval-logloss:0.461136\n",
      "[388]\ttrain-logloss:0.322095\teval-logloss:0.461121\n",
      "[389]\ttrain-logloss:0.321944\teval-logloss:0.461097\n",
      "[390]\ttrain-logloss:0.321702\teval-logloss:0.461080\n",
      "[391]\ttrain-logloss:0.321451\teval-logloss:0.461054\n",
      "[392]\ttrain-logloss:0.321300\teval-logloss:0.461026\n",
      "[393]\ttrain-logloss:0.321005\teval-logloss:0.460978\n",
      "[394]\ttrain-logloss:0.320824\teval-logloss:0.460970\n",
      "[395]\ttrain-logloss:0.320664\teval-logloss:0.460944\n",
      "[396]\ttrain-logloss:0.320403\teval-logloss:0.460916\n",
      "[397]\ttrain-logloss:0.320222\teval-logloss:0.460902\n",
      "[398]\ttrain-logloss:0.320105\teval-logloss:0.460862\n",
      "[399]\ttrain-logloss:0.319844\teval-logloss:0.460819\n",
      "[400]\ttrain-logloss:0.319653\teval-logloss:0.460780\n",
      "[401]\ttrain-logloss:0.319429\teval-logloss:0.460748\n",
      "[402]\ttrain-logloss:0.319233\teval-logloss:0.460750\n",
      "[403]\ttrain-logloss:0.319008\teval-logloss:0.460709\n",
      "[404]\ttrain-logloss:0.318861\teval-logloss:0.460676\n",
      "[405]\ttrain-logloss:0.318704\teval-logloss:0.460652\n",
      "[406]\ttrain-logloss:0.318563\teval-logloss:0.460644\n",
      "[407]\ttrain-logloss:0.318254\teval-logloss:0.460598\n",
      "[408]\ttrain-logloss:0.318133\teval-logloss:0.460590\n",
      "[409]\ttrain-logloss:0.317882\teval-logloss:0.460530\n",
      "[410]\ttrain-logloss:0.317731\teval-logloss:0.460504\n",
      "[411]\ttrain-logloss:0.317498\teval-logloss:0.460461\n",
      "[412]\ttrain-logloss:0.317372\teval-logloss:0.460444\n",
      "[413]\ttrain-logloss:0.317185\teval-logloss:0.460420\n",
      "[414]\ttrain-logloss:0.316932\teval-logloss:0.460386\n",
      "[415]\ttrain-logloss:0.316786\teval-logloss:0.460375\n",
      "[416]\ttrain-logloss:0.316472\teval-logloss:0.460328\n",
      "[417]\ttrain-logloss:0.316228\teval-logloss:0.460285\n",
      "[418]\ttrain-logloss:0.316123\teval-logloss:0.460283\n",
      "[419]\ttrain-logloss:0.315947\teval-logloss:0.460267\n",
      "[420]\ttrain-logloss:0.315799\teval-logloss:0.460264\n",
      "[421]\ttrain-logloss:0.315554\teval-logloss:0.460240\n",
      "[422]\ttrain-logloss:0.315404\teval-logloss:0.460216\n",
      "[423]\ttrain-logloss:0.315227\teval-logloss:0.460201\n",
      "[424]\ttrain-logloss:0.315100\teval-logloss:0.460183\n",
      "[425]\ttrain-logloss:0.314853\teval-logloss:0.460145\n",
      "[426]\ttrain-logloss:0.314640\teval-logloss:0.460154\n",
      "[427]\ttrain-logloss:0.314375\teval-logloss:0.460124\n",
      "[428]\ttrain-logloss:0.314172\teval-logloss:0.460089\n",
      "[429]\ttrain-logloss:0.314029\teval-logloss:0.460097\n",
      "[430]\ttrain-logloss:0.313903\teval-logloss:0.460079\n",
      "[431]\ttrain-logloss:0.313769\teval-logloss:0.460065\n",
      "[432]\ttrain-logloss:0.313589\teval-logloss:0.460041\n",
      "[433]\ttrain-logloss:0.313422\teval-logloss:0.460017\n",
      "[434]\ttrain-logloss:0.313154\teval-logloss:0.459997\n",
      "[435]\ttrain-logloss:0.313053\teval-logloss:0.459992\n",
      "[436]\ttrain-logloss:0.312821\teval-logloss:0.459967\n",
      "[437]\ttrain-logloss:0.312689\teval-logloss:0.459961\n",
      "[438]\ttrain-logloss:0.312531\teval-logloss:0.459952\n",
      "[439]\ttrain-logloss:0.312295\teval-logloss:0.459912\n",
      "[440]\ttrain-logloss:0.312069\teval-logloss:0.459885\n",
      "[441]\ttrain-logloss:0.311860\teval-logloss:0.459855\n",
      "[442]\ttrain-logloss:0.311695\teval-logloss:0.459850\n",
      "[443]\ttrain-logloss:0.311515\teval-logloss:0.459843\n",
      "[444]\ttrain-logloss:0.311389\teval-logloss:0.459833\n",
      "[445]\ttrain-logloss:0.311166\teval-logloss:0.459812\n",
      "[446]\ttrain-logloss:0.311037\teval-logloss:0.459813\n",
      "[447]\ttrain-logloss:0.310736\teval-logloss:0.459774\n",
      "[448]\ttrain-logloss:0.310498\teval-logloss:0.459741\n",
      "[449]\ttrain-logloss:0.310378\teval-logloss:0.459738\n",
      "[450]\ttrain-logloss:0.310172\teval-logloss:0.459712\n",
      "[451]\ttrain-logloss:0.310008\teval-logloss:0.459700\n",
      "[452]\ttrain-logloss:0.309919\teval-logloss:0.459692\n",
      "[453]\ttrain-logloss:0.309843\teval-logloss:0.459692\n",
      "[454]\ttrain-logloss:0.309700\teval-logloss:0.459683\n",
      "[455]\ttrain-logloss:0.309596\teval-logloss:0.459675\n",
      "[456]\ttrain-logloss:0.309393\teval-logloss:0.459672\n",
      "[457]\ttrain-logloss:0.309122\teval-logloss:0.459613\n",
      "[458]\ttrain-logloss:0.308875\teval-logloss:0.459584\n",
      "[459]\ttrain-logloss:0.308619\teval-logloss:0.459540\n",
      "[460]\ttrain-logloss:0.308479\teval-logloss:0.459538\n",
      "[461]\ttrain-logloss:0.308300\teval-logloss:0.459537\n",
      "[462]\ttrain-logloss:0.308106\teval-logloss:0.459515\n",
      "[463]\ttrain-logloss:0.307960\teval-logloss:0.459496\n",
      "[464]\ttrain-logloss:0.307853\teval-logloss:0.459488\n",
      "[465]\ttrain-logloss:0.307651\teval-logloss:0.459444\n",
      "[466]\ttrain-logloss:0.307471\teval-logloss:0.459398\n",
      "[467]\ttrain-logloss:0.307236\teval-logloss:0.459400\n",
      "[468]\ttrain-logloss:0.307119\teval-logloss:0.459378\n",
      "[469]\ttrain-logloss:0.306936\teval-logloss:0.459382\n",
      "[470]\ttrain-logloss:0.306697\teval-logloss:0.459359\n",
      "[471]\ttrain-logloss:0.306526\teval-logloss:0.459339\n",
      "[472]\ttrain-logloss:0.306428\teval-logloss:0.459326\n",
      "[473]\ttrain-logloss:0.306318\teval-logloss:0.459325\n",
      "[474]\ttrain-logloss:0.306244\teval-logloss:0.459314\n",
      "[475]\ttrain-logloss:0.306044\teval-logloss:0.459306\n",
      "[476]\ttrain-logloss:0.305853\teval-logloss:0.459269\n",
      "[477]\ttrain-logloss:0.305592\teval-logloss:0.459230\n",
      "[478]\ttrain-logloss:0.305450\teval-logloss:0.459229\n",
      "[479]\ttrain-logloss:0.305308\teval-logloss:0.459195\n",
      "[480]\ttrain-logloss:0.305061\teval-logloss:0.459159\n",
      "[481]\ttrain-logloss:0.304807\teval-logloss:0.459134\n",
      "[482]\ttrain-logloss:0.304514\teval-logloss:0.459123\n",
      "[483]\ttrain-logloss:0.304354\teval-logloss:0.459106\n",
      "[484]\ttrain-logloss:0.304277\teval-logloss:0.459105\n",
      "[485]\ttrain-logloss:0.304192\teval-logloss:0.459098\n",
      "[486]\ttrain-logloss:0.304102\teval-logloss:0.459094\n",
      "[487]\ttrain-logloss:0.304024\teval-logloss:0.459091\n",
      "[488]\ttrain-logloss:0.303964\teval-logloss:0.459080\n",
      "[489]\ttrain-logloss:0.303812\teval-logloss:0.459052\n",
      "[490]\ttrain-logloss:0.303642\teval-logloss:0.459035\n",
      "[491]\ttrain-logloss:0.303528\teval-logloss:0.459020\n",
      "[492]\ttrain-logloss:0.303456\teval-logloss:0.459011\n",
      "[493]\ttrain-logloss:0.303312\teval-logloss:0.459000\n",
      "[494]\ttrain-logloss:0.303083\teval-logloss:0.458961\n",
      "[495]\ttrain-logloss:0.302907\teval-logloss:0.458928\n",
      "[496]\ttrain-logloss:0.302694\teval-logloss:0.458910\n",
      "[497]\ttrain-logloss:0.302479\teval-logloss:0.458865\n",
      "[498]\ttrain-logloss:0.302294\teval-logloss:0.458862\n",
      "[499]\ttrain-logloss:0.302207\teval-logloss:0.458856\n",
      "[500]\ttrain-logloss:0.302074\teval-logloss:0.458847\n",
      "[501]\ttrain-logloss:0.301933\teval-logloss:0.458829\n",
      "[502]\ttrain-logloss:0.301818\teval-logloss:0.458809\n",
      "[503]\ttrain-logloss:0.301760\teval-logloss:0.458813\n",
      "[504]\ttrain-logloss:0.301477\teval-logloss:0.458781\n",
      "[505]\ttrain-logloss:0.301392\teval-logloss:0.458758\n",
      "[506]\ttrain-logloss:0.301189\teval-logloss:0.458728\n",
      "[507]\ttrain-logloss:0.301123\teval-logloss:0.458719\n",
      "[508]\ttrain-logloss:0.300957\teval-logloss:0.458714\n",
      "[509]\ttrain-logloss:0.300917\teval-logloss:0.458718\n",
      "[510]\ttrain-logloss:0.300826\teval-logloss:0.458721\n",
      "[511]\ttrain-logloss:0.300774\teval-logloss:0.458724\n",
      "[512]\ttrain-logloss:0.300596\teval-logloss:0.458707\n",
      "[513]\ttrain-logloss:0.300377\teval-logloss:0.458691\n",
      "[514]\ttrain-logloss:0.300207\teval-logloss:0.458672\n",
      "[515]\ttrain-logloss:0.299969\teval-logloss:0.458655\n",
      "[516]\ttrain-logloss:0.299801\teval-logloss:0.458651\n",
      "[517]\ttrain-logloss:0.299596\teval-logloss:0.458653\n",
      "[518]\ttrain-logloss:0.299399\teval-logloss:0.458657\n",
      "[519]\ttrain-logloss:0.299308\teval-logloss:0.458655\n",
      "[520]\ttrain-logloss:0.299147\teval-logloss:0.458638\n",
      "[521]\ttrain-logloss:0.299037\teval-logloss:0.458635\n",
      "[522]\ttrain-logloss:0.298836\teval-logloss:0.458624\n",
      "[523]\ttrain-logloss:0.298670\teval-logloss:0.458607\n",
      "[524]\ttrain-logloss:0.298528\teval-logloss:0.458597\n",
      "[525]\ttrain-logloss:0.298465\teval-logloss:0.458586\n",
      "[526]\ttrain-logloss:0.298330\teval-logloss:0.458581\n",
      "[527]\ttrain-logloss:0.298136\teval-logloss:0.458569\n",
      "[528]\ttrain-logloss:0.298092\teval-logloss:0.458576\n",
      "[529]\ttrain-logloss:0.297866\teval-logloss:0.458560\n",
      "[530]\ttrain-logloss:0.297718\teval-logloss:0.458549\n",
      "[531]\ttrain-logloss:0.297608\teval-logloss:0.458546\n",
      "[532]\ttrain-logloss:0.297366\teval-logloss:0.458550\n",
      "[533]\ttrain-logloss:0.297219\teval-logloss:0.458547\n",
      "[534]\ttrain-logloss:0.297127\teval-logloss:0.458536\n",
      "[535]\ttrain-logloss:0.297051\teval-logloss:0.458527\n",
      "[536]\ttrain-logloss:0.296983\teval-logloss:0.458524\n",
      "[537]\ttrain-logloss:0.296839\teval-logloss:0.458513\n",
      "[538]\ttrain-logloss:0.296649\teval-logloss:0.458508\n",
      "[539]\ttrain-logloss:0.296584\teval-logloss:0.458502\n",
      "[540]\ttrain-logloss:0.296422\teval-logloss:0.458498\n",
      "[541]\ttrain-logloss:0.296293\teval-logloss:0.458500\n",
      "[542]\ttrain-logloss:0.296121\teval-logloss:0.458484\n",
      "[543]\ttrain-logloss:0.296094\teval-logloss:0.458478\n",
      "[544]\ttrain-logloss:0.295960\teval-logloss:0.458481\n",
      "[545]\ttrain-logloss:0.295851\teval-logloss:0.458468\n",
      "[546]\ttrain-logloss:0.295644\teval-logloss:0.458455\n",
      "[547]\ttrain-logloss:0.295484\teval-logloss:0.458429\n",
      "[548]\ttrain-logloss:0.295302\teval-logloss:0.458416\n",
      "[549]\ttrain-logloss:0.295148\teval-logloss:0.458407\n",
      "[550]\ttrain-logloss:0.295002\teval-logloss:0.458409\n",
      "[551]\ttrain-logloss:0.294906\teval-logloss:0.458405\n",
      "[552]\ttrain-logloss:0.294840\teval-logloss:0.458406\n",
      "[553]\ttrain-logloss:0.294769\teval-logloss:0.458419\n",
      "[554]\ttrain-logloss:0.294646\teval-logloss:0.458421\n",
      "[555]\ttrain-logloss:0.294521\teval-logloss:0.458402\n",
      "[556]\ttrain-logloss:0.294361\teval-logloss:0.458403\n",
      "[557]\ttrain-logloss:0.294205\teval-logloss:0.458396\n",
      "[558]\ttrain-logloss:0.294138\teval-logloss:0.458395\n",
      "[559]\ttrain-logloss:0.294100\teval-logloss:0.458396\n",
      "[560]\ttrain-logloss:0.293924\teval-logloss:0.458378\n",
      "[561]\ttrain-logloss:0.293746\teval-logloss:0.458375\n",
      "[562]\ttrain-logloss:0.293611\teval-logloss:0.458374\n",
      "[563]\ttrain-logloss:0.293439\teval-logloss:0.458354\n",
      "[564]\ttrain-logloss:0.293399\teval-logloss:0.458345\n",
      "[565]\ttrain-logloss:0.293327\teval-logloss:0.458335\n",
      "[566]\ttrain-logloss:0.293194\teval-logloss:0.458318\n",
      "[567]\ttrain-logloss:0.293028\teval-logloss:0.458304\n",
      "[568]\ttrain-logloss:0.292884\teval-logloss:0.458302\n",
      "[569]\ttrain-logloss:0.292755\teval-logloss:0.458288\n",
      "[570]\ttrain-logloss:0.292607\teval-logloss:0.458278\n",
      "[571]\ttrain-logloss:0.292545\teval-logloss:0.458277\n",
      "[572]\ttrain-logloss:0.292355\teval-logloss:0.458257\n",
      "[573]\ttrain-logloss:0.292309\teval-logloss:0.458243\n",
      "[574]\ttrain-logloss:0.292263\teval-logloss:0.458243\n",
      "[575]\ttrain-logloss:0.292101\teval-logloss:0.458247\n",
      "[576]\ttrain-logloss:0.291932\teval-logloss:0.458240\n",
      "[577]\ttrain-logloss:0.291792\teval-logloss:0.458236\n",
      "[578]\ttrain-logloss:0.291742\teval-logloss:0.458230\n",
      "[579]\ttrain-logloss:0.291597\teval-logloss:0.458218\n",
      "[580]\ttrain-logloss:0.291537\teval-logloss:0.458223\n",
      "[581]\ttrain-logloss:0.291505\teval-logloss:0.458218\n",
      "[582]\ttrain-logloss:0.291412\teval-logloss:0.458218\n",
      "[583]\ttrain-logloss:0.291351\teval-logloss:0.458217\n",
      "[584]\ttrain-logloss:0.291210\teval-logloss:0.458215\n",
      "[585]\ttrain-logloss:0.291141\teval-logloss:0.458213\n",
      "[586]\ttrain-logloss:0.291097\teval-logloss:0.458210\n",
      "[587]\ttrain-logloss:0.291039\teval-logloss:0.458209\n",
      "[588]\ttrain-logloss:0.290890\teval-logloss:0.458196\n",
      "[589]\ttrain-logloss:0.290855\teval-logloss:0.458186\n",
      "[590]\ttrain-logloss:0.290713\teval-logloss:0.458180\n",
      "[591]\ttrain-logloss:0.290522\teval-logloss:0.458176\n",
      "[592]\ttrain-logloss:0.290500\teval-logloss:0.458177\n",
      "[593]\ttrain-logloss:0.290390\teval-logloss:0.458173\n",
      "[594]\ttrain-logloss:0.290299\teval-logloss:0.458166\n",
      "[595]\ttrain-logloss:0.290197\teval-logloss:0.458165\n",
      "[596]\ttrain-logloss:0.290006\teval-logloss:0.458156\n",
      "[597]\ttrain-logloss:0.289882\teval-logloss:0.458138\n",
      "[598]\ttrain-logloss:0.289736\teval-logloss:0.458142\n",
      "[599]\ttrain-logloss:0.289633\teval-logloss:0.458127\n",
      "[600]\ttrain-logloss:0.289554\teval-logloss:0.458116\n"
     ]
    }
   ],
   "source": [
    "for i, (train_idx, test_idx) in enumerate(skf):\n",
    "    print i\n",
    "    X = train.iloc[train_idx]\n",
    "    Y = train.iloc[test_idx]\n",
    "    Xtarget = target.iloc[train_idx]\n",
    "    Ytarget = target.iloc[test_idx]\n",
    "    \n",
    "    indice = 0\n",
    "    #run models\n",
    "    \n",
    "    preds = []\n",
    "    predstest = []\n",
    "    \n",
    "    for model in clfs:\n",
    "        print indice\n",
    "        if model == 'xgboost-1':\n",
    "            dtrain = xgb.DMatrix(X, label = Xtarget)\n",
    "            dcv = xgb.DMatrix(Y, label = Ytarget)\n",
    "            watchlist = [(dtrain,'train'), (dcv,'eval')]\n",
    "            clf_1 = xgb.train(xgboost_params_1, dtrain, num_boost_round = xgb_rounds, evals = watchlist, early_stopping_rounds = 100)\n",
    "            preds.append(clf_1.predict(dcv, ntree_limit = clf_1.best_iteration))\n",
    "        elif model == 'xgboost-2':\n",
    "            dtrain = xgb.DMatrix(X, label = Xtarget)\n",
    "            dcv = xgb.DMatrix(Y, label = Ytarget)\n",
    "            watchlist = [(dtrain,'train'), (dcv,'eval')]\n",
    "            clf_2 = xgb.train(xgboost_params_2, dtrain, num_boost_round = xgb_rounds, evals = watchlist, early_stopping_rounds = 100)\n",
    "            preds.append(clf_2.predict(dcv, ntree_limit = clf_2.best_iteration))\n",
    "        elif model == 'xgboost-3':\n",
    "            dtrain = xgb.DMatrix(X, label = Xtarget)\n",
    "            dcv = xgb.DMatrix(Y, label = Ytarget)\n",
    "            watchlist = [(dcv,'eval'), (dtrain,'train')]\n",
    "            clf_3 = xgb.train(xgboost_params_3, dtrain, num_boost_round = xgb_rounds, evals = watchlist, early_stopping_rounds = 100)\n",
    "            preds.append(clf_3.predict(dcv, ntree_limit = clf_3.best_iteration))\n",
    "        elif model == 'xgboost-4':\n",
    "            dtrain = xgb.DMatrix(X, label = Xtarget)\n",
    "            dcv = xgb.DMatrix(Y, label = Ytarget)\n",
    "            watchlist = [(dcv,'eval'), (dtrain,'train')]\n",
    "            clf_4 = xgb.train(xgboost_params_4, dtrain, num_boost_round = xgb_rounds, evals = watchlist, early_stopping_rounds = 100)\n",
    "            preds.append(clf_4.predict(dcv, ntree_limit = clf_4.best_iteration))\n",
    "        else:\n",
    "            # pdb.set_trace()\n",
    "            model.fit(X, Xtarget)\n",
    "            preds.append(model.predict_proba(Y)[:,1])\n",
    "            \n",
    "        print('model ',indice,': loss=',metrics.log_loss(Ytarget, preds[indice]))\n",
    "        \n",
    "        if model == 'xgboost-1':\n",
    "            dtest = xgb.DMatrix(test)\n",
    "            predstest.append(clf_1.predict(dtest, ntree_limit = clf_1.best_iteration))\n",
    "        elif model == 'xgboost-2':\n",
    "            dtest = xgb.DMatrix(test)\n",
    "            predstest.append(clf_2.predict(dtest, ntree_limit = clf_2.best_iteration))\n",
    "        elif model == 'xgboost-3':\n",
    "            dtest = xgb.DMatrix(test)\n",
    "            predstest.append(clf_3.predict(dtest, ntree_limit = clf_3.best_iteration))\n",
    "        elif model == 'xgboost-4':\n",
    "            dtest = xgb.DMatrix(test)\n",
    "            predstest.append(clf_4.predict(dtest, ntree_limit = clf_4.best_iteration))\n",
    "        else:\n",
    "            predstest.append(model.predict_proba(test)[:,1])\n",
    "        \n",
    "        indice += 1\n",
    "        \n",
    "    # find best weights\n",
    "    step = 0.1 * (1./len(preds))\n",
    "    print(\"step: \", step)\n",
    "    poidsref = np.zeros(len(preds))\n",
    "    poids = np.zeros(len(preds))\n",
    "    poidsreftemp = np.zeros(len(preds))\n",
    "    poidsref = poidsref + 1./len(preds)\n",
    "        \n",
    "    bestpoids = poidsref.copy()\n",
    "    blend_cv = np.zeros(len(preds[0]))\n",
    "        \n",
    "    for k in range(0, len(preds), 1):\n",
    "        blend_cv = blend_cv + bestpoids[k] * preds[k]\n",
    "        bestscore = metrics.log_loss(Ytarget.values, blend_cv)\n",
    "            \n",
    "    getting_better_score = True\n",
    "    while getting_better_score:\n",
    "        getting_better_score = False\n",
    "        for i in range(0, len(preds), 1):\n",
    "            poids = poidsref\n",
    "            if poids[i] - step>-step:\n",
    "                # decrease weight in position i\n",
    "                poids[i] -= step\n",
    "                for j in range(0, len(preds), 1):\n",
    "                    if j != i:\n",
    "                        if poids[j] + step <= 1:\n",
    "                            # try an increase in position j\n",
    "                            poids[j] += step\n",
    "                            # score new weights\n",
    "                            blend_cv = np.zeros(len(preds[0]))\n",
    "                            for k in range(0, len(preds), 1):\n",
    "                                blend_cv = blend_cv + poids[k] * preds[k]\n",
    "                            actualscore = metrics.log_loss(Ytarget.values, blend_cv)\n",
    "                            # if better, keep it\n",
    "                            if actualscore < bestscore:\n",
    "                                bestscore = actualscore\n",
    "                                bestpoids = poids.copy()\n",
    "                                getting_better_score = True\n",
    "                            poids[j] -= step\n",
    "                poids[i] += step\n",
    "        poidsref = bestpoids.copy()\n",
    "            \n",
    "    print(\"weights: \", bestpoids)\n",
    "    print(\"optimal blend loss: \", bestscore)\n",
    "    \n",
    "    blend_to_submit = np.zeros(len(predstest[0]))\n",
    "    for i in range(0, len(preds), 1):\n",
    "        blend_to_submit = blend_to_submit + bestpoids[i] * predstest[i]\n",
    "        \n",
    "    final_submit.append(blend_to_submit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit = np.mean(final_submit, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# submit\n",
    "submission = pd.read_csv('sample_submission.csv')\n",
    "submission.PredictedProb = submit\n",
    "submission.to_csv('blend.csv', index = False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "('model ', 0, ': loss=', 0.45807333631960728)\n",
    "('model ', 1, ': loss=', 0.45738420810920294)\n",
    "2\n",
    "('model ', 2, ': loss=', 0.46115888303822938)\n",
    "3\n",
    "('model ', 3, ': loss=', 0.46498613044159121)\n",
    "('step: ', 0.025)\n",
    "('weights: ', array([ 0.175,  0.45 ,  0.05 ,  0.325]))\n",
    "('optimal blend loss: ', 0.45266126735562168)\n",
    "\n",
    "('model ', 0, ': loss=', 0.45966951363016784)\n",
    "('model ', 1, ': loss=', 0.45912098790454337)\n",
    "2\n",
    "('model ', 2, ': loss=', 0.461942080022192)\n",
    "3\n",
    "('model ', 3, ': loss=', 0.46559816307938889)\n",
    "('step: ', 0.025)\n",
    "('weights: ', array([ 0.2 ,  0.4 ,  0.05,  0.35]))\n",
    "('optimal blend loss: ', 0.4539998147310404)\n",
    "\n",
    "('model ', 0, ': loss=', 0.45537199386772054)\n",
    "('model ', 1, ': loss=', 0.45515158559120583)\n",
    "2\n",
    "('model ', 2, ': loss=', 0.45800846329910971)\n",
    "3\n",
    "('model ', 3, ': loss=', 0.46348930937496224)\n",
    "('step: ', 0.025)\n",
    "('weights: ', array([ 0.325,  0.3  ,  0.1  ,  0.275]))\n",
    "('optimal blend loss: ', 0.45065059665837853)\n",
    "\n",
    "('model ', 0, ': loss=', 0.45695240536879483)\n",
    "('model ', 1, ': loss=', 0.45618553407359258)\n",
    "2\n",
    "('model ', 2, ': loss=', 0.46077977610966792)\n",
    "3\n",
    "('model ', 3, ': loss=', 0.46363260708359821)\n",
    "('step: ', 0.025)\n",
    "('weights: ', array([ 0.375,  0.275, -0.025,  0.375]))\n",
    "('optimal blend loss: ', 0.45091668856370953)\n",
    "\n",
    "('model ', 0, ': loss=', 0.45705147775043548)\n",
    "('model ', 1, ': loss=', 0.45681671811250585)\n",
    "2\n",
    "('model ', 2, ': loss=', 0.45968281846410747)\n",
    "3\n",
    "('model ', 3, ': loss=', 0.46388611307964395)\n",
    "('step: ', 0.025)\n",
    "('weights: ', array([ 0.15,  0.45,  0.1 ,  0.3 ]))\n",
    "('optimal blend loss: ', 0.45193566006337776)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}